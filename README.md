# üöÄ Documentaci√≥n del Pipeline de Detecci√≥n de Violencia

Este documento detalla la arquitectura y el flujo de datos del *pipeline* de detecci√≥n de violencia. Sirve como una gu√≠a t√©cnica para entender el proyecto y como un manual para la implementaci√≥n en producci√≥n.

## 1. üèõÔ∏è Estructura de Carpetas

El proyecto est√° dividido en dos partes principales: un paquete de Python (`model_api`) que contiene todo el c√≥digo fuente, y un conjunto de scripts en la ra√≠z (`run_app.py`, `test_websocket.py`) que ejecutan el *pipeline*.

```
PROYECTO_URBANSENTINEL/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ run_app.py
‚îú‚îÄ‚îÄ test_websocket.py
‚îú‚îÄ‚îÄ venv_api/
‚îî‚îÄ‚îÄ model_api/
    ‚îú‚îÄ‚îÄ __pycache__/
    ‚îú‚îÄ‚îÄ api/
    ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
    ‚îÇ   ‚îú‚îÄ‚îÄ connection_manager.py
    ‚îÇ   ‚îú‚îÄ‚îÄ event_manager.py
    ‚îÇ   ‚îî‚îÄ‚îÄ main.py
    ‚îú‚îÄ‚îÄ config/
    ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
    ‚îÇ   ‚îî‚îÄ‚îÄ config.py
    ‚îú‚îÄ‚îÄ data/
    ‚îÇ   ‚îú‚îÄ‚îÄ clips_guardados/
    ‚îÇ   ‚îú‚îÄ‚îÄ logs_eventos/
    ‚îÇ   ‚îî‚îÄ‚îÄ videos_prueba/
    ‚îú‚îÄ‚îÄ onnx_model/
    ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
    ‚îÇ   ‚îú‚îÄ‚îÄ onnx_detector.py
    ‚îÇ   ‚îú‚îÄ‚îÄ swin3d_t.onnx
    ‚îÇ   ‚îî‚îÄ‚îÄ swin3d_t.onnx.data
    ‚îú‚îÄ‚îÄ processing/
    ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
    ‚îÇ   ‚îî‚îÄ‚îÄ video_processor.py
    ‚îî‚îÄ‚îÄ services/
        ‚îú‚îÄ‚îÄ __pycache__/
        ‚îú‚îÄ‚îÄ stream_reader/
        ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
        ‚îÇ   ‚îú‚îÄ‚îÄ base_reader.py
        ‚îÇ   ‚îî‚îÄ‚îÄ file_reader.py
        ‚îú‚îÄ‚îÄ camera_worker.py
        ‚îú‚îÄ‚îÄ event_recorder.py
        ‚îî‚îÄ‚îÄ inference_service.py

```

## 2. üåä Flujo de Datos del Pipeline (Paso a Paso)

El *pipeline* est√° dise√±ado en 3 procesos principales para m√°xima eficiencia:
1.  **Ingesta (CPU):** Un proceso `camera_worker.py` por cada c√°mara.
2.  **Inferencia (GPU):** Un √∫nico proceso `inference_service.py` que sirve a todas las c√°maras.
3.  **L√≥gica (API):** Un proceso `api/main.py` que maneja las decisiones y la comunicaci√≥n.

**El flujo de una predicci√≥n es el siguiente:**

1.  **`run_app.py`** (El Orquestador) se ejecuta. Inicia los procesos de GPU y API. Luego, lee su lista de c√°maras (4 en nuestra prueba) e inicia 4 procesos `camera_worker.py`.
2.  **`camera_worker.py`** (Proceso CPU)
    * Inicia un `file_reader.py` que comienza a leer una lista de videos de prueba.
    * Mantiene dos b√∫feres de frames: uno para inferencia (`inference_buffer`) y uno para pre-grabaci√≥n (`pre_roll_buffer`).
    * Gracias a un `time.sleep()`, el *worker* se frena a s√≠ mismo para simular 30 FPS.
    * Cada 16 frames (`STRIDE`), el *worker* llama a `preprocess_clip()`.
3.  **`video_processor.py`** (L√≥gica de CPU)
    * Recibe los frames (ej. 32 frames de un video de 30 FPS).
    * Realiza el preprocesamiento: `resize`, `crop`, y `normalize`.
    * Devuelve un tensor *√∫nico* con forma `(3, 32, 224, 224)`.
4.  **`camera_worker.py`** (de vuelta)
    * Recibe el tensor y lo valida con `np.isfinite()` para asegurarse de que no est√© corrupto (evitando crasheos de GPU).
    * Si es v√°lido, pone el tensor y su ID (ej. `("cam_01", tensor)`) en la `inference_queue`.
5.  **`inference_service.py`** (Proceso GPU)
    * Est√° escuchando la `inference_queue`.
    * Toma el clip `("cam_01", tensor)`. **(Procesa 1 a la vez)**.
    * Lo expande a un lote de 1: `(1, 3, 32, 224, 224)`.
    * Ejecuta `detector.predict_batch()`.
    * (En la primera llamada) `onnx_detector.py` carga el modelo en la GPU ("Lazy Loading").
    * Pone el resultado (ej. `("cam_01", [0.9, 0.1, 0.1])`) en la `results_queue`.
6.  **`event_manager.py`** (Proceso API)
    * Est√° escuchando la `results_queue`.
    * Recibe `("cam_01", [0.9, 0.1, 0.1])`.
    * **Transmite (Broadcast)** la predicci√≥n por WebSocket a todos los clientes que escuchen a `cam_01`.
    * **Toma una Decisi√≥n:** Compara `0.9` con el `ALERT_THRESHOLD` (0.7).
    * Como es `> 0.7` y el estado de la c√°mara era `IDLE`, pone dos mensajes en la `control_queue` de `cam_01`:
        1.  El *string* `"START_RECORDING"`.
        2.  El *array* `[0.9, 0.1, 0.1]`.
7.  **`camera_worker.py`** (de vuelta)
    * Detecta los mensajes en su `control_queue`.
    * Al recibir `"START_RECORDING"`, crea una instancia de `EventRecorder` (un hilo) y le pasa el b√∫fer de pre-grabaci√≥n (los 5 segundos *antes* de la detecci√≥n).
    * Llama a `recorder.start()`, iniciando el hilo de grabaci√≥n.
    * Al recibir `[0.9, 0.1, 0.1]`, actualiza su variable `last_known_probs`.
8.  **`event_recorder.py`** (Hilo de Grabaci√≥n)
    * El bucle principal del *worker* (que sigue a 30 FPS) ahora solo pone el frame y las `last_known_probs` en la `frame_queue` del grabador (esto es instant√°neo).
    * El hilo de `EventRecorder` saca el frame de su cola, lo escribe en el disco (`cv2.VideoWriter`) y aplica su propio `time.sleep()` para mantenerse a 30 FPS, evitando as√≠ los errores de FFmpeg.

---

## 3. üìö Gu√≠a de Archivos y L√≥gica

Aqu√≠ se detalla qu√© hace cada archivo en el proyecto.

### Grupo 1: Configuraci√≥n (`/model_api/config/`)

* **`config.py`**
    * **Qu√© hace:** Es el "panel de control" de todo el proyecto. Contiene todas las constantes y variables m√°gicas en un solo lugar.
    * **L√≥gica Clave:** Define `CLASSES` (las etiquetas), `CLIP_LEN` (32 frames), `TARGET_FPS` (30 FPS), `STRIDE` (16 frames), `PRE_ROLL_SECONDS` (5 segundos), `ALERT_THRESHOLD` (0.7 o 70%) y la lista de `INFERENCE_PROVIDERS` (CUDA, Dml, CPU).

### Grupo 2: M√≥dulos de IA y Procesamiento (`/model_api/onnx_model/` y `/model_api/processing/`)

* **`onnx_detector.py`**
    * **Qu√© hace:** Una clase "envoltorio" (wrapper) que maneja el modelo ONNX.
    * **L√≥gica Clave:** Usa **Lazy Loading**: no carga el modelo en `__init__`. El modelo solo se carga en la GPU (`_load_model()`) la primera vez que se llama a `predict_batch()`. Esto es crucial para evitar *deadlocks* de CUDA con `multiprocessing`. Lee `config.INFERENCE_PROVIDERS` para decidir si usar NVIDIA (CUDA), AMD (DML) o CPU.
* **`video_processor.py`**
    * **Qu√© hace:** Una librer√≠a de funciones puras. Su √∫nica funci√≥n, `preprocess_clip()`, convierte una lista de frames de video en un tensor listo para la IA.
    * **L√≥gica Clave:** La l√≥gica de normalizaci√≥n de FPS est√° aqu√≠ (`np.linspace`). Toma una lista de frames (ej. 64 frames de un video de 60 FPS) y la "muestrea" a 32 frames (`config.CLIP_LEN`), replicando la forma en que el modelo fue entrenado. Devuelve un tensor de forma `(3, 32, 224, 224)`.

### Grupo 3: M√≥dulos de I/O (`/model_api/services/stream_reader/` y `event_recorder.py`)

* **`base_reader.py`**
    * **Qu√© hace:** Define la "interfaz" o "contrato" que todos los lectores de video deben seguir. Fuerza a que todos tengan los m√©todos `read()`, `get_fps()` y `release()`.
* **`file_reader.py`**
    * **Qu√© hace:** Es el lector de video que usamos para **pruebas locales**.
    * **L√≥gica Clave:** Acepta una **lista** de rutas de video. Reproduce el video 1, luego el video 2, etc. Cuando termina la lista, vuelve al video 1 y repite (looping), simulando un *stream* de c√°mara infinito.
* **`event_recorder.py`**
    * **Qu√© hace:** Es el grabador de video. Est√° dise√±ado para ejecutarse como un **hilo** (`threading.Thread`) separado.
    * **L√≥gica Clave:** Al crearse, escribe el b√∫fer de pre-rollo. Luego, su hilo `run()` se queda en un bucle sacando frames de una `queue.Queue` y escribi√©ndolos en el disco. Implementa su propio `time.sleep()` para sincronizarse a 30 FPS y evitar las advertencias de FFmpeg. `close()` detiene el hilo de forma segura y guarda el `.json` final.

### Grupo 4: Los Servicios (Workers) (`/model_api/services/`)

* **`camera_worker.py`**
    * **Qu√© hace:** Es el "Ingestor de CPU" y el *proceso* m√°s complejo. Se ejecuta uno por cada c√°mara.
    * **L√≥gica Clave:**
        1.  **Ingesta:** Usa un `stream_reader` (como `FileReader`) para leer frames.
        2.  **Control de FPS:** Usa `time.sleep(delay_por_frame)` para frenarse a s√≠ mismo a los 30 FPS de la fuente.
        3.  **Procesamiento:** Mantiene el `inference_buffer` y llama a `preprocess_clip()` cada 16 frames (`STRIDE`).
        4.  **Validaci√≥n:** Comprueba el tensor resultante con `np.isfinite()` para proteger a la GPU de datos corruptos.
        5.  **Control de Grabaci√≥n:** Escucha la `control_queue`. Inicia/Detiene el hilo `EventRecorder` y reenv√≠a los *arrays* de probabilidades a la cola del grabador para que se guarden en el `.json`.
* **`inference_service.py`**
    * **Qu√© hace:** Es el "Coraz√≥n de la GPU". Solo se ejecuta **un** proceso de este tipo en todo el sistema.
    * **L√≥gica Clave:** **Procesa clips de uno en uno (Batch Size = 1)**. Esta fue la correcci√≥n clave para evitar los errores de `Reshape node` del modelo ONNX. Su l√≥gica es un bucle simple: `inference_queue.get()`, `np.expand_dims()` (para crear un lote de 1), `detector.predict_batch()`, y `results_queue.put()`.

### Grupo 5: La API (`/model_api/api/`)

* **`connection_manager.py`**
    * **Qu√© hace:** Una clase simple que gestiona los clientes de WebSocket. Mantiene un diccionario que mapea un `camera_id` a una lista de conexiones (navegadores) que est√°n viendo esa c√°mara.
* **`event_manager.py`**
    * **Qu√© hace:** Es el "Cerebro L√≥gico" de la aplicaci√≥n. Se ejecuta como una tarea de fondo (`async`) dentro de la API.
    * **L√≥gica Clave (Detecci√≥n y Decisi√≥n):**
        1.  Aqu√≠ es donde **se detecta la violencia por primera vez** (`is_violence_detected = any(p > config.ALERT_THRESHOLD ...)`).
        2.  Transmite **todas** las predicciones (violentas o no) al *frontend* v√≠a WebSocket.
        3.  Implementa la "m√°quina de estados" (`IDLE` <-> `RECORDING`).
        4.  Env√≠a los comandos `"START_RECORDING"`, `"STOP_RECORDING"` y los *arrays* de probabilidades a la `control_queue` del *worker* correspondiente.
* **`main.py`**
    * **Qu√© hace:** Define la aplicaci√≥n FastAPI (`app = FastAPI(...)`) y los *endpoints*.
    * **L√≥gica Clave:** Define el *endpoint* `/ws/{camera_id}` al que se conecta el *frontend* (React). Usa una funci√≥n `lifespan` (que reemplaza al `@app.on_event("startup")` obsoleto) para iniciar la tarea de fondo `event_manager_task` cuando se enciende el servidor.

### Grupo 6: Los Lanzadores (`/`)

* **`run_app.py`**
    * **Qu√© hace:** Es el **√∫nico script que debes ejecutar** para iniciar todo el *backend*.
    * **L√≥gica Clave:**
        1.  Establece `multiprocessing.set_start_method("spawn")` (cr√≠tico para CUDA en Windows).
        2.  Crea las `multiprocessing.Queue` (colas de procesos).
        3.  Escanea los videos de prueba y los divide en 4 listas.
        4.  Inicia el `inference_service` (1 Proceso).
        5.  Inicia los 4 `camera_worker` (4 Procesos).
        6.  "Inyecta" las colas en las variables globales del m√≥dulo `api_main`.
        7.  Inicia el servidor `uvicorn` en el proceso principal, que a su vez carga `api/main.py`.
* **`test_websocket.py`**
    * **Qu√© hace:** Un script de prueba para simular ser el *frontend*.
    * **L√≥gica Clave:** Usa `asyncio.gather()` para conectarse a los 4 *endpoints* WebSocket (`cam_01` a `cam_04`) en paralelo y muestra todas las predicciones que recibe.

---

## 4. üöÄ Gu√≠a de Ejecuci√≥n (Para tu Compa√±ero con AMD)

Esta gu√≠a explica c√≥mo ejecutar el *pipeline* de prueba actual en una PC con una **GPU AMD (RX 6600)** usando **Python 3.12.10**.

### 4.1. Configuraci√≥n del Entorno

1.  **Instalar Python:** Aseg√∫rate de tener **Python 3.12.10** instalado. (La versi√≥n 3.10 o 3.11 tambi√©n funciona).
2.  **Instalar Drivers:** Aseg√∫rate de tener los √∫ltimos *drivers* **AMD Adrenalin** para la RX 6600.
3.  **Clonar el Proyecto:** `git clone ...`
4.  **Crear Entorno Virtual:**
    ```bash
    # (Aseg√∫rate de que 'python' apunte a tu instalaci√≥n de 3.12)
    python -m venv venv_api
    ```
5.  **Activar Entorno:**
    * En Windows: `.\venv_api\Scripts\Activate.ps1`
6.  **Instalar Dependencias:** (Esta es la parte m√°s importante)

    ```bash
    # Instalar la versi√≥n de ONNX Runtime para AMD (DirectML)
    pip install onnxruntime-directml
    
    # Instalar el resto de dependencias
    pip install numpy opencv-python fastapi "uvicorn[standard]" websockets
    ```
    **¬°No instales `onnxruntime-gpu`!** Esa librer√≠a es solo para NVIDIA (CUDA).

### 4.2. Modificaciones de C√≥digo (¬°No se necesita ninguna!)

**No necesitas modificar ning√∫n archivo.**

El *pipeline* ya est√° configurado para manejar AMD. La magia est√° en estos dos archivos:

1.  **`model_api/config/config.py`:**
    * La variable `INFERENCE_PROVIDERS` ya incluye la opci√≥n de AMD:
    * `['CUDAExecutionProvider', 'DmlExecutionProvider', 'CPUExecutionProvider']`

2.  **`model_api/onnx_model/onnx_detector.py`:**
    * Este archivo lee esa lista.
    * En tu PC (NVIDIA), encontrar√° `CUDAExecutionProvider` y lo usar√°.
    * En la PC de tu compa√±ero (AMD), fallar√° en encontrar CUDA, pero luego **encontrar√° `DmlExecutionProvider` (DirectML) y lo usar√° autom√°ticamente.**

### 4.3. Ejecutar la Prueba de 4 C√°maras

1.  **Abrir Terminal 1 (Backend):**
    * Activa el entorno (`.\venv_api\Scripts\Activate.ps1`).
    * Ejecuta el lanzador:
        ```bash
        python run_app.py
        ```
    * Espera a que todos los procesos se inicien. Ver√°s los logs de los 4 *workers* y el `InferenceService`. Espera hasta que veas el log final:
        > `INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)`

2.  **Abrir Terminal 2 (Cliente):**
    * Activa el entorno (`.\venv_api\Scripts\Activate.ps1`).
    * Ejecuta el script de prueba de WebSocket:
        ```bash
        python test_websocket.py
        ```

3.  **Verificar Resultados:**
    * La **Terminal 1** (`run_app.py`) deber√≠a mostrar el log `[Detector] Modelo cargado y listo en: DmlExecutionProvider` (confirmando que est√° usando la GPU de AMD).
    * La **Terminal 2** (`test_websocket.py`) deber√≠a empezar a mostrar el *stream* de predicciones en JSON de las 4 c√°maras.

---

## 5. üõ†Ô∏è Gu√≠a de Implementaci√≥n en Producci√≥n

Para mover este *pipeline* de la "prueba con archivos" a la "aplicaci√≥n real", tu compa√±ero del *backend* debe modificar 3 √°reas principales:

### 1. Reemplazar el Lector de Video (Ingesta)

* [cite_start]**Paso 8 del Pipeline (`pipeline.md` [cite: 664-670]):** La tarea m√°s importante es implementar la decodificaci√≥n por hardware (NVDEC para NVIDIA o VAAPI/DXVA2 para AMD).
* **Acci√≥n:**
    1.  Crear el archivo `model_api/services/stream_reader/rtsp_reader.py`.
    2.  Esta clase debe usar `GStreamer` o `FFmpeg` con Python para conectarse a una URL `rtsp://` y decodificar el video usando la GPU, no la CPU.
    3.  Actualizar el `camera_worker.py` para que reconozca `reader_type="rtsp"`:
        ```python
        # (En camera_worker.py)
        if reader_type == "file":
            stream_reader = FileReader(source_path)
        elif reader_type == "rtsp":
            stream_reader = RtspReader(source_path) # <-- A√ëADIR ESTO
        ```

### 2. Conectar la Base de Datos (Orquestaci√≥n)

* **Problema:** `run_app.py` actualmente "hardcodea" la lista de c√°maras leyendo videos locales.
* **Acci√≥n:**
    1.  Modificar el bloque `if __name__ == "__main__":` en `run_app.py`.
    2.  **Eliminar** la funci√≥n `get_video_files()` y la l√≥gica `CAMERAS_TO_RUN`.
    3.  [cite_start]En su lugar, a√±adir la l√≥gica para **consultar la base de datos PostgreSQL** (definida en tu OB2).
    4.  El *script* debe hacer algo como: `SELECT id_conexion, url_rtsp FROM conexiones WHERE estado='ACTIVA'`.
    5.  Luego, construir la lista `CAMERAS_TO_RUN` din√°micamente a partir de esa consulta:
        ```python
        # (Ejemplo en run_app.py)
        # db_cameras = ... (c√≥digo para consultar la BD)
        CAMERAS_TO_RUN = []
        for cam in db_cameras:
            CAMERAS_TO_RUN.append({
                "id": cam.id_conexion,
                "type": "rtsp",  # <-- Usar el nuevo lector
                "path": cam.url_rtsp # <-- Usar la URL de la BD
            })
        
        # El resto del script (crear colas, iniciar procesos) sigue igual.
        ```

### 3. Implementar los Endpoints REST (API)

* **Problema:** `api/main.py` actualmente solo tiene el *endpoint* de WebSocket (`/ws/{camera_id}`) y un *endpoint* ra√≠z (`/`).
* **Acci√≥n:**
    1.  Tu compa√±ero debe a√±adir aqu√≠ todos los *endpoints* **REST API** que la aplicaci√≥n web necesita, como se define en tu arquitectura (`OB2- Dise√±o del proyecto.pdf`)[cite: 292, 342, 348, 399].
    2.  Ejemplos:
        * `@app.post("/login")` (Gesti√≥n de usuarios) [cite: 293, 306, 381]
        * `@app.get("/cameras")` (Gesti√≥n de c√°maras) [cite: 298, 308, 384]
        * `@app.get("/reports")` (Gesti√≥n de reportes) [cite: 302, 798]
    3.  Estos *endpoints* contendr√°n la l√≥gica de negocio para leer y escribir en la base de datos PostgreSQL.